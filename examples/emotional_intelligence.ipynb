{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_context, get_response\n",
    "from artifacts import (\n",
    "    linkedin_notifications_artifact_content,\n",
    "    slack_notifications_artifact_content,\n",
    "    informal_background_artifact_content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Emotional Intelligence in Social Media Interactions using AI-driven Response Generation**\n",
    "\n",
    "This notebook demonstrates how to implement **emotionally intelligent response generation** for **social media interactions**. The goal is to classify message tones and generate appropriate responses that align with the emotional context of the conversation. By leveraging **context-aware prompting and structured reasoning**, this approach enhances **engagement, user experience, and sentiment alignment** in automated responses.\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "- Utilizes **emotion classification** to adapt responses based on message tone and platform context.\n",
    "- Employs a **structured reasoning framework** for generating empathetic and contextually appropriate replies.\n",
    "- Uses the **`inflection_3_pi` model**, optimized for emotional intelligence and excels in scenarios such as customer support chatbots.\n",
    "- The model performs the following tasks:\n",
    "  - **Classifies emotional tone** of incoming messages across different social media platforms.\n",
    "  - **Adjusts response style** based on detected sentiment and conversation flow.\n",
    "  - Generates **structured, adaptive replies** that enhance engagement and mitigate negative interactions.\n",
    "  - Ensures responses align with **platform-specific norms and user expectations**.\n",
    "\n",
    "This approach improves **customer experience**, minimizes misinterpretations, and enhances brand communication consistency across social media channels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"inflection_3_pi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_prompt_linkedin = \"\"\"\n",
    "You are an AI assistant developed by Inflection AI, designed to help users craft professional responses on LinkedIn.\n",
    "\n",
    "# Your Purpose\n",
    "1. You will receive a userâ€™s LinkedIn notifications and messages, along with the specific message they want a reply to.\n",
    "2. Based on the provided information, generate a professional and concise response.\n",
    "3. Use formal language without emojis to maintain professionalism.\n",
    "4. Keep responses short and to the point.\n",
    "5. Remember, this is a LinkedIn message, not an email.\n",
    "6. Only return the response itself, do not include the original message or any additional context.\n",
    "\n",
    "# Output Format - XML\n",
    "You will respond in a valid XML format with the <parts> and <response> tags following the output format. You don't need to provide explanation or any other information, just return response.\n",
    "\n",
    "# Format of the Output\n",
    "<parts>\n",
    "    <response>Response to the message.</response>\n",
    "</parts>\n",
    "\"\"\"\n",
    "\n",
    "system_instruction_prompt_slack = \"\"\"\n",
    "You are an AI assistant developed by Inflection AI, designed to help users craft responses on Slack.\n",
    "\n",
    "# Your Purpose\n",
    "1. You will receive a userâ€™s Slack notifications and messages, along with the specific message they want a reply to.\n",
    "2. Based on the provided information, generate a friendly and concise response.\n",
    "3. Use casual language and make sure to include emojis to make the message more engaging and human like.\n",
    "4. Keep responses brief and to the point.\n",
    "5. Remember, this is a Slack message, NOT an email.\n",
    "6. Only return the response itself, do not include the original message or any additional context.\n",
    "\n",
    "# Output Format - XML\n",
    "You will respond in a valid XML format with the <parts> and <response> tags following the output format. You don't need to provide explanation or any other information, just return response.\n",
    "\n",
    "# Format of the Output\n",
    "<parts>\n",
    "    <response>Response to the message.</response>\n",
    "</parts>\n",
    "\"\"\"\n",
    "\n",
    "system_instruction_prompt_informal = \"\"\"\n",
    "You are an AI assistant developed by Inflection AI, designed to provide customer support in an informal, friendly manner.\n",
    "\n",
    "# Your Purpose\n",
    "1. You will be given a userâ€™s question and background information about the company, mission, and FAQ.\n",
    "2. Process the userâ€™s message and generate a concise, engaging response using the provided background information.\n",
    "3. Use casual, human like language with emojis to enhance friendliness.\n",
    "4. Keep responses brief and to the point.\n",
    "5. Return response ONLY in XML format, do not include the original message or extra context.\n",
    "\n",
    "# Output Format - XML\n",
    "You MUST respond in a valid XML format with the <parts> and <response> tags following the output format. You don't need to provide explanation or any other information, just return response.\n",
    "\n",
    "# Format of the Output\n",
    "<parts>\n",
    "    <response>Response to the message.</response>\n",
    "</parts>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenario: Respond to different types of messages on different social media platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test: test_get_response\n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference:Inflection AI API request took 4981.85 ms (Model=[inflection_3_pi]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Response: \u001b[0m Hi John, thank you for your interest in connecting! Iâ€™d be delighted to discuss the future of AI with you. Looking forward to exchanging ideas!\n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference:Inflection AI API request took 4799.61 ms (Model=[inflection_3_pi]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Response: \u001b[0m Thanks, Sean! I appreciate the feedback. Looking forward to discussing the next steps in tomorrowâ€™s meeting.\n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference:Inflection AI API request took 5123.20 ms (Model=[inflection_3_pi]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Response: \u001b[0m \n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n",
      "All tests passed successfully! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "async def test_get_response():\n",
    "    print(\"Starting test: test_get_response\")\n",
    "    print(\"+*\"*20)\n",
    "    message = linkedin_notifications_artifact_content + \"\\n # Which Notification Or Message The Human Wants You To Draft A Reply To: New Connection Request: John Smith\"\n",
    "    context = get_context(system_instruction_prompt_linkedin, message)\n",
    "    result = await get_response(context, [\"response\"], model=model)\n",
    "\n",
    "    print(f\"{color.BOLD} Response: {color.END} {result['response']}\")\n",
    "    print(\"+*\"*20)\n",
    "\n",
    "    message = slack_notifications_artifact_content + \"\\n # Which Notification Or Message The Human Wants You To Draft A Reply To: Message from Sean White\"\n",
    "    context = get_context(system_instruction_prompt_linkedin, message)\n",
    "    result = await get_response(context, [\"response\"], model=model)\n",
    "\n",
    "    print(f\"{color.BOLD} Response: {color.END} {result['response']}\")\n",
    "    print(\"+*\"*20)\n",
    "\n",
    "    message = \"Company's background information:\" + informal_background_artifact_content + \"\\n  User's question: What's company's mission?\"\n",
    "    context = get_context(system_instruction_prompt_linkedin, message)\n",
    "    result = await get_response(context, [\"response\"], model=model)\n",
    "\n",
    "    print(f\"{color.BOLD} Response: {color.END} {result['response']}\")\n",
    "    print(\"+*\"*20)\n",
    "\n",
    "    print(\"All tests passed successfully! ðŸ™Œ\")\n",
    "\n",
    "# Run the test\n",
    "await test_get_response()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
