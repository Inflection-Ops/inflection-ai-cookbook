{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "\n",
    "from utils import get_context\n",
    "from inference import fetch as fetch_inflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Retrieval-Augmented Generation (RAG) for Context-Aware AI Agents Using Inflection AI**\n",
    "\n",
    "This notebook demonstrates how to leverage **Retrieval-Augmented Generation (RAG)** to build **context-aware AI agents** using **Inflection AI**. The approach enhances response quality by integrating **retrieval-based context** with **deep learning models** for intelligent reasoning and decision-making.\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "- Implements **context-aware AI agents** using **RAG**, ensuring responses are grounded in retrieved knowledge.\n",
    "- Utilizes **Inflection AI API** to enhance reasoning and inference.\n",
    "- Leverages **transformers-based models** for tokenization and embedding.\n",
    "- Key functionalities include:\n",
    "  - **Chunking** input text for efficient retrieval.\n",
    "  - **Embedding** textual data for semantic similarity matching.\n",
    "  - **Retrieving relevant context** before generating a response.\n",
    "  - **Generating coherent and informed answers** using RAG.\n",
    "- Integrates **ModernBERT-base** for embedding and retrieval tasks.\n",
    "- Demonstrates how **Inflection AI can be used effectively when RAG is established**, ensuring responses are well-supported by retrieved evidence.\n",
    "\n",
    "This structured approach enables AI agents to **retrieve, reason, and respond** with **high accuracy and contextual awareness**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571c4e4e01ed4eaf9d9400d7ea3021b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc65c1224dd46b0a1f10df94abbd43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ececfe0be5473490ee0ae936761d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc0df33a2f0942deb118ba07bac4f392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968cbe1d4fec423494ca83509fdfe415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name =  \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Electric vehicles (EVs) are becoming more popular due to their efficiency and environmental benefits. Charging infrastructure is expanding worldwide.\",\n",
    "    \"Quantum computing uses principles of quantum mechanics to perform calculations at speeds unattainable by classical computers.\",\n",
    "    \"Renewable energy sources like solar and wind power are key to reducing carbon emissions and combating climate change.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(texts: list, max_tokens: int =10 , encoding_name: str=\"cl100k_base\") -> list:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    chunks = []\n",
    "    for text in texts:\n",
    "        tokens = enc.encode(text)\n",
    "        for i in range(0, len(tokens), max_tokens):\n",
    "            chunk = tokens[i:i + max_tokens]\n",
    "            chunks.append(enc.decode(chunk))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "processed_chunks = get_chunks(texts)\n",
    "print(len(processed_chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Encodes a piece of text using ModernBERT\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].squeeze().numpy()  # Take [CLS] token representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "chunk_embeddings = np.array([encode_text(chunk) for chunk in processed_chunks])  # Store embeddings in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store chunks for lookup\n",
    "chunk_dict = {i: processed_chunks[i] for i in range(len(processed_chunks))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query: str, k: int=4) -> list:\n",
    "    \"\"\"Encodes query, retrieves top k matching chunks using cosine similarity\"\"\"\n",
    "    query_embedding = encode_text(query).reshape(1, -1)  # Encode query\n",
    "    distances = cdist(query_embedding, chunk_embeddings, metric=\"cosine\")  # Compute cosine similarity\n",
    "    indices = np.argsort(distances)[0][:k]  # Get top-k closest chunks\n",
    "    \n",
    "    retrieved_texts = [chunk_dict[idx] for idx in indices]\n",
    "    return retrieved_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Chunks: ['Electric vehicles (EVs) are becoming more popular', ' is expanding worldwide.', ' are key to reducing carbon emissions and combating climate change', ' calculations at speeds unattainable by classical computers']\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the benefits of electric vehicles?\"\n",
    "retrieved_chunks = retrieve_top_k(question)\n",
    "print(\"Retrieved Chunks:\", retrieved_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_instruction_prompt = \"\"\"\n",
    "You are a helpful assistant. Your task is to answer the user's question strictly based on the provided context.\n",
    "\n",
    "## Instructions:\n",
    "- You will be given a question and a context.\n",
    "- Your answer must be based only on the provided context. Do not include any external information or assumptions.\n",
    "- If the answer is not in the context, state that explicitly. Do not attempt to infer or fabricate an answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenario: Retrieve relevant information and use it to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class color:\n",
    "    BOLD = '\\033[1m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test: test_rag_enabled_agents\n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference:Inflection AI API request took 4159.67 ms (Model=[inflection_3_pi]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Question: \u001b[0m What are the benefits of electric vehicles?\n",
      "\u001b[1m Response: \u001b[0m Electric vehicles (EVs) have several benefits, primarily being key to reducing carbon emissions and combating climate change. Although the provided context doesn't detail other advantages, EVs are generally known for lower operating costs, reduced noise pollution, and lower maintenance requirements compared to traditional combustion engine vehicles.\n",
      "+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*+*\n",
      "Test completed successfully! ðŸ™Œ\n"
     ]
    }
   ],
   "source": [
    "async def test_rag_enabled_agents():\n",
    "    print(\"Starting test: test_rag_enabled_agents\")\n",
    "    print(\"+*\"*20)\n",
    "\n",
    "    query = f\"Query: {question}\\nRetrieved context: {retrieved_chunks}\"\n",
    "    context = get_context(system_instruction_prompt, query)\n",
    "    response = await fetch_inflection(context)\n",
    "    \n",
    "    print(f\"{color.BOLD} Question: {color.END} {question}\")\n",
    "    print(f\"{color.BOLD} Response: {color.END} {response}\")\n",
    "    print(\"+*\"*20)\n",
    "\n",
    "    print(\"Test completed successfully! ðŸ™Œ\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "await test_rag_enabled_agents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
